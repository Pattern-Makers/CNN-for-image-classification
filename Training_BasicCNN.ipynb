{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE6IpNgi-E_e"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZGi-8C-a0S",
        "outputId": "a5339a81-6bd9-4c37-d2b4-85a86be511f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-11 05:55:15--  https://archive.ics.uci.edu/static/public/773/defungi.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘defungi.zip.1’\n",
            "\n",
            "defungi.zip.1           [   <=>              ]  87.53M  9.66MB/s               "
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/773/defungi.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnAW3Tsv-nM_"
      },
      "outputs": [],
      "source": [
        "!unzip defungi.zip -d /content/data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8Kf2GV6_JO6"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations (you can modify these as needed)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the images to 224x224 (common for pretrained models)\n",
        "    transforms.ToTensor(),          # Convert the images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet standards\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "DATA_DIR = '/content/data'\n",
        "\n",
        "dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
        "\n",
        "# Check class-to-index mapping\n",
        "print(\"Class to Index Mapping:\", dataset.class_to_idx)\n",
        "\n",
        "# Check the length of the dataset and an example data point\n",
        "print(\"Dataset size:\", len(dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfZ_D2OdAO1w"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7G0WD9uA4kQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Extract a sample image and its label from the dataset\n",
        "image, label = dataset[0]\n",
        "\n",
        "def plot_image(image, label):\n",
        "  # Reverse the normalization to display the image correctly\n",
        "  unnormalize = transforms.Normalize(\n",
        "      mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
        "      std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
        "  )\n",
        "\n",
        "  image = unnormalize(image)\n",
        "\n",
        "  # Convert the image tensor to a NumPy array and transpose it for plotting\n",
        "  image = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "  # Plot the image using matplotlib\n",
        "  plt.imshow(image)\n",
        "  plt.title(f'Class label: {label}')\n",
        "  plt.axis('off')  # Hide the axes\n",
        "  plt.show()\n",
        "\n",
        "plot_image(image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDowW-RSB0hW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Calculate sizes for each split\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.6 * total_size)\n",
        "val_size = int(0.2 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders for each subset\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DQlAB2aDzzD",
        "outputId": "e188f532-4cca-4439-8699-3104e351c955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Train Loss: 1.4187, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 2/20, Train Loss: 1.4165, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 3/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 4/20, Train Loss: 1.4172, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 5/20, Train Loss: 1.4175, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 6/20, Train Loss: 1.4165, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 7/20, Train Loss: 1.4168, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 8/20, Train Loss: 1.4175, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 9/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 10/20, Train Loss: 1.4175, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 11/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 12/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 13/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 14/20, Train Loss: 1.4172, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 15/20, Train Loss: 1.4177, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 16/20, Train Loss: 1.4175, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 17/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 18/20, Train Loss: 1.4170, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 19/20, Train Loss: 1.4165, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Epoch 20/20, Train Loss: 1.4165, Val Loss: 1.4325, Val Accuracy: 0.4737\n",
            "Test Loss: 1.4242, Test Accuracy: 0.4792\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN architecture\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * 26 * 26, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.005\n",
        "num_epochs =20\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = BasicCNN(num_classes=5, dropout_rate=dropout_rate)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
        "          f\"Val Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {correct/total:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dIkwIHhp2f_Q",
        "outputId": "87a8352e-21ee-4294-e64a-75e2ee90500d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.4242, Test Accuracy: 0.4792\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {correct/total:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}